{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "compueter_vision_and_cnn.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAgZsqdIet6X"
      },
      "source": [
        "# Get the data"
      ],
      "id": "wAgZsqdIet6X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDjZhQS0tq57"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "id": "WDjZhQS0tq57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "749ce3a8"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip\n",
        "\n",
        "# unzip the downloaded file\n",
        "zip_ref = zipfile.ZipFile(\"pizza_steak.zip\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "id": "749ce3a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRdYDzBGgmPN"
      },
      "source": [
        "## Inspect the data"
      ],
      "id": "gRdYDzBGgmPN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1N50dZlfjX_"
      },
      "source": [
        "!ls pizza_steak/test/pizza"
      ],
      "id": "C1N50dZlfjX_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AfTdsQlgzNc"
      },
      "source": [
        "import os\n",
        "\n",
        "# Walk through pizza_steak directory\n",
        "for dirpath, dirnames, filenames in os.walk(\"pizza_steak\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'\")"
      ],
      "id": "1AfTdsQlgzNc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1LYhMRQhfcl"
      },
      "source": [
        "num_steak_images_train = len(os.listdir(\"pizza_steak/train/steak\"))\n",
        "num_steak_images_train"
      ],
      "id": "h1LYhMRQhfcl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r53StWKuil5T"
      },
      "source": [
        "# Get the classnames programmatically\n",
        "import pathlib\n",
        "import numpy as np\n",
        "\n",
        "data_dir = pathlib.Path(\"pizza_steak/train\")\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")]))\n",
        "class_names = class_names[1:]\n",
        "class_names"
      ],
      "id": "r53StWKuil5T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k70ZycbSjLrc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "\n",
        "def view_random_image(target_dir, target_class):\n",
        "  target_folder = target_dir + target_class\n",
        "  random_image = random.sample(os.listdir(target_folder), 1)\n",
        "\n",
        "  img = mpimg.imread(target_folder + \"/\" + random_image[0])\n",
        "  plt.imshow(img)\n",
        "  plt.title(target_class)\n",
        "  plt.axis(\"off\");\n",
        "\n",
        "  print(f\"Image shape: {img.shape}\")\n",
        "\n",
        "  return img"
      ],
      "id": "k70ZycbSjLrc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C7hQ8KHs2pH"
      },
      "source": [
        "# View random image\n",
        "img = view_random_image(target_class=\"pizza\", target_dir=\"pizza_steak/train/\")"
      ],
      "id": "1C7hQ8KHs2pH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYKz0y4YtFIf"
      },
      "source": [
        "tf.constant(img)"
      ],
      "id": "FYKz0y4YtFIf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzOEAYHZtj0m"
      },
      "source": [
        "img.shape # returns width, height, color channels"
      ],
      "id": "yzOEAYHZtj0m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwVEVXuRuF1k"
      },
      "source": [
        "# Get all the pixel values between 0 & 1\n",
        "img/255"
      ],
      "id": "cwVEVXuRuF1k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yagw3chlvKVj"
      },
      "source": [
        "## An End to End Example"
      ],
      "id": "Yagw3chlvKVj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKoU0MVvum6-"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# set seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Preprocess data\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Setup paths to our data directories\n",
        "train_dir = \"/content/pizza_steak/train\"\n",
        "test_dir = \"pizza_steak/test\"\n",
        "\n",
        "# Import data from dirs and turn it into batches\n",
        "train_data = train_datagen.flow_from_directory(directory=train_dir,\n",
        "                                               batch_size=32,\n",
        "                                               target_size=(224, 224),\n",
        "                                               class_mode=\"binary\",\n",
        "                                               seed=42)\n",
        "\n",
        "valid_data = valid_datagen.flow_from_directory(directory=test_dir,\n",
        "                                               batch_size=32,\n",
        "                                               target_size=(224,224),\n",
        "                                               class_mode=\"binary\",\n",
        "                                               seed=42)\n",
        "\n",
        "# Build a CNN model\n",
        "model_1 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Conv2D(filters=10,\n",
        "                         kernel_size=3,\n",
        "                         activation=\"relu\",\n",
        "                         input_shape=(224, 224, 3)),\n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
        "  tf.keras.layers.MaxPool2D(pool_size=2,\n",
        "                            padding=\"valid\"),\n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
        "  tf.keras.layers.MaxPool2D(2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=([\"accuracy\"]))\n",
        "\n",
        "history_1 = model_1.fit(train_data, epochs=10,\n",
        "                        steps_per_epoch=len(train_data),\n",
        "                        validation_data=valid_data,\n",
        "                        validation_steps=len(valid_data))"
      ],
      "id": "eKoU0MVvum6-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EnLzLPfrr9K"
      },
      "source": [
        "model_1.summary()"
      ],
      "id": "3EnLzLPfrr9K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av2-tqXtF1ts"
      },
      "source": [
        "pd.DataFrame(history_1.history).plot()"
      ],
      "id": "av2-tqXtF1ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa-_aV-CpGFM"
      },
      "source": [
        "## Using the same model as before"
      ],
      "id": "oa-_aV-CpGFM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7S_MpHFu90v"
      },
      "source": [
        "# Get random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model_2 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(224, 224, 3)),\n",
        "  tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "\n",
        "model_2.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "history_2 = model_2.fit(train_data,\n",
        "            epochs=10,\n",
        "            steps_per_epoch=len(train_data),\n",
        "            validation_data=valid_data,\n",
        "            validation_steps=len(valid_data))"
      ],
      "id": "_7S_MpHFu90v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lubDdvsArkC8"
      },
      "source": [
        "model_2.summary()"
      ],
      "id": "lubDdvsArkC8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOzw_TvTqcFv"
      },
      "source": [
        "pd.DataFrame(history_2.history).plot()"
      ],
      "id": "QOzw_TvTqcFv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZxsTQONrJxJ"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model_3 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(224, 224, 3)),\n",
        "  tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_3.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history_3 = model_3.fit(\n",
        "    train_data,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_data),\n",
        "    validation_data=valid_data,\n",
        "    validation_steps=len(valid_data)\n",
        ")"
      ],
      "id": "1ZxsTQONrJxJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBvFICbCthHL"
      },
      "source": [
        "model_3.summary()"
      ],
      "id": "RBvFICbCthHL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDXr3dMytrKS"
      },
      "source": [
        "pd.DataFrame(history_3.history).plot()"
      ],
      "id": "iDXr3dMytrKS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXQkcomoty_I"
      },
      "source": [
        "model_1.summary()"
      ],
      "id": "VXQkcomoty_I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqhJ4X0Wvdym"
      },
      "source": [
        "## Binary Classification: Break Down"
      ],
      "id": "dqhJ4X0Wvdym"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n103prSuvLS6"
      },
      "source": [
        "# Visualize the data\n",
        "plt.figure()\n",
        "plt.subplot(1, 2, 1)\n",
        "steak_img = view_random_image(\"pizza_steak/train/\", \"steak\")\n",
        "plt.subplot(1, 2, 2)\n",
        "pizza_img = view_random_image(\"pizza_steak/train/\", \"pizza\")"
      ],
      "id": "n103prSuvLS6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7asPy8icxUXl"
      },
      "source": [
        "## Preprocessing the data"
      ],
      "id": "7asPy8icxUXl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzJa9gD7wZ0k"
      },
      "source": [
        "# Define directory dataset paths\n",
        "train_dir = \"pizza_steak/train/\"\n",
        "test_dir = \"pizza_steak/test/\""
      ],
      "id": "zzJa9gD7wZ0k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0asnDVxxi10"
      },
      "source": [
        "# Turn data into batches\n",
        "\n",
        "# Craete train and test data generators and rescale the data\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)"
      ],
      "id": "p0asnDVxxi10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYXQc9HlxwQB"
      },
      "source": [
        "# Load in out image data from directories and turn them into batches\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(directory=train_dir, # Target directory if images\n",
        "                                               target_size=(224, 224), # Target size of images (H, W)\n",
        "                                               class_mode=\"binary\", # Type of the data\n",
        "                                               batch_size=32) # Size of the mini batches\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(directory=test_dir,\n",
        "                                             target_size=(224, 224),\n",
        "                                             class_mode=\"binary\",\n",
        "                                             batch_size=32)"
      ],
      "id": "sYXQc9HlxwQB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaJnKFo_0Plq"
      },
      "source": [
        "# Get a sample of a train data batch\n",
        "images, labels = train_data.next() # get the nect batch of data\n",
        "len(images), len(labels)"
      ],
      "id": "gaJnKFo_0Plq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3i2li_dbjdG"
      },
      "source": [
        "# How many batches are there?\n",
        "len(train_data)"
      ],
      "id": "H3i2li_dbjdG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgLhcV-Zbr6M"
      },
      "source": [
        "# Get the first two images\n",
        "images[:2], images[0].shape"
      ],
      "id": "OgLhcV-Zbr6M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6uRRYiKb5R-"
      },
      "source": [
        "index = random.randint(0, 31)\n",
        "plt.imshow(images[index])\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Pizza\" if labels[index] == 0.0 else \"Stack\")"
      ],
      "id": "s6uRRYiKb5R-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biV0pHtleHdV"
      },
      "source": [
        "## Create a CNN model (Start with baseline)"
      ],
      "id": "biV0pHtleHdV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM9G-8-CdRoG"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation\n",
        "from tensorflow.keras import Sequential"
      ],
      "id": "LM9G-8-CdRoG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY77bnUdXXKZ"
      },
      "source": [
        "model_4 = Sequential([\n",
        "  Conv2D(filters=32,\n",
        "         kernel_size=3,\n",
        "         strides=1,\n",
        "         padding=\"valid\",\n",
        "         activation=\"relu\",\n",
        "         input_shape=(224, 224, 3)),\n",
        "  Conv2D(10, 3, activation=\"relu\"),\n",
        "  Conv2D(10, 3, activation=\"relu\"),\n",
        "  Flatten(),\n",
        "  Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_4.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "history_4 = model_4.fit(train_data,\n",
        "            epochs=10,\n",
        "            steps_per_epoch=len(train_data),\n",
        "            validation_data=test_data,\n",
        "            validation_steps=len(test_data))"
      ],
      "id": "sY77bnUdXXKZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6fv9FfDgBTq"
      },
      "source": [
        "pd.DataFrame(history_4.history).plot()"
      ],
      "id": "e6fv9FfDgBTq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL7a3I82iTxu"
      },
      "source": [
        "model_4.summary()"
      ],
      "id": "LL7a3I82iTxu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLzhsQwX42Dn"
      },
      "source": [
        "## Evaluating a model"
      ],
      "id": "eLzhsQwX42Dn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY1DODYx3p9L"
      },
      "source": [
        "model_4.evaluate(test_data)"
      ],
      "id": "cY1DODYx3p9L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlVZ0JHO477N"
      },
      "source": [
        "# Plot the validation and training curves separately\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns separate loss curves\n",
        "  \"\"\"\n",
        "  loss = history.history[\"loss\"]\n",
        "  val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "  accuracy = history.history[\"accuracy\"]\n",
        "  val_accuracy = history.history[\"val_accuracy\"]\n",
        "\n",
        "  epochs = range(len(history.history[\"loss\"]))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.figure(figsize=(18,7))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(epochs, loss, label=\"Training Loss\")\n",
        "  plt.plot(epochs, val_loss, label=\"Validation Loss\")\n",
        "  plt.title(\"loss\")\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot Accuracy\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(epochs, accuracy, label=\"Training Accuracy\")\n",
        "  plt.plot(epochs, val_accuracy, label=\"Validation Accuracy\")\n",
        "  plt.title(\"accuracy\")\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.legend();"
      ],
      "id": "GlVZ0JHO477N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpDvc3Vg6ln_"
      },
      "source": [
        "plot_loss_curves(history_4)"
      ],
      "id": "EpDvc3Vg6ln_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdCF7QqZ7yHF"
      },
      "source": [
        "## Adjust the model parameters"
      ],
      "id": "JdCF7QqZ7yHF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8njXz2e6oSr"
      },
      "source": [
        "model_5 = Sequential([\n",
        "  Conv2D(32, 3, activation=\"relu\", input_shape=(224, 224, 3)),\n",
        "  MaxPool2D(pool_size=2),\n",
        "  Conv2D(10, 3, activation=\"relu\", input_shape=(224, 224, 3)),\n",
        "  MaxPool2D(pool_size=2),\n",
        "  Conv2D(19, 3, activation=\"relu\", input_shape=(224, 224, 3)),\n",
        "  MaxPool2D(pool_size=2),\n",
        "  Flatten(),\n",
        "  Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "id": "r8njXz2e6oSr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OshxdC9w80aV"
      },
      "source": [
        "model_5.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=Adam(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "id": "OshxdC9w80aV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0M-r8hf9CC8"
      },
      "source": [
        "history_5 = model_5.fit(\n",
        "    train_data,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_data),\n",
        "    validation_data=test_data,\n",
        "    validation_steps=len(test_data)\n",
        ")"
      ],
      "id": "Y0M-r8hf9CC8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utj66lz99-Ch"
      },
      "source": [
        "model_5.summary()"
      ],
      "id": "Utj66lz99-Ch",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BvTv7qV9Xvu"
      },
      "source": [
        "plot_loss_curves(history_5)"
      ],
      "id": "4BvTv7qV9Xvu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfAhfBF7YgAI"
      },
      "source": [
        "## Adding Data Augmentation"
      ],
      "id": "jfAhfBF7YgAI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81bVlGNwYopw"
      },
      "source": [
        "# Data with Augmentation\n",
        "train_datagen_augmented = ImageDataGenerator(rescale=1/255.,\n",
        "                                             rotation_range=0.2,\n",
        "                                             shear_range=0.2,\n",
        "                                             zoom_range=0.2,\n",
        "                                             width_shift_range=0.2,\n",
        "                                             height_shift_range=0.2,\n",
        "                                             horizontal_flip=True)\n",
        "# Data without Augmentation\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "# Test data without Augmentation\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)"
      ],
      "id": "81bVlGNwYopw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPjkm6J4ZjKk"
      },
      "source": [
        "print(\"Augmented Train Dataset\")\n",
        "train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,\n",
        "                                                                   target_size=(224, 224),\n",
        "                                                                   batch_size=32,\n",
        "                                                                   class_mode=\"binary\",\n",
        "                                                                   seed=42)\n",
        "print(\"Non-Augmented training data\")\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               target_size=(224, 224),\n",
        "                                               batch_size=32,\n",
        "                                               class_mode=\"binary\",\n",
        "                                               seed=42)\n",
        "\n",
        "print(\"Non-augmented test data\")\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                             target_size=(224, 224),\n",
        "                                             batch_size=32,\n",
        "                                             class_mode=\"binary\")"
      ],
      "id": "rPjkm6J4ZjKk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li7xlj5hbHVT"
      },
      "source": [
        "# Get sample data batch\n",
        "images, labels = train_data.next()\n",
        "augmented_images, augmented_labels = train_data_augmented.next()"
      ],
      "id": "li7xlj5hbHVT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GQY2ZOkb7ax"
      },
      "source": [
        "index = random.randint(0, 31)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(images[index])\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Pizza\" if labels[index] == 0.0 else \"Steak\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(augmented_images[index])\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Augmented Pizza\" if labels[index] == 0.0 else \"Augmented Steak\");"
      ],
      "id": "3GQY2ZOkb7ax",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgYDpC6XcAgZ"
      },
      "source": [
        "## Training a model in Augmented data"
      ],
      "id": "bgYDpC6XcAgZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bherNl-9ek1y"
      },
      "source": [
        "model_6 = Sequential([\n",
        "  Conv2D(filters=32, kernel_size=3, strides=1, padding=\"valid\", input_shape=(224, 224, 3), activation=\"relu\"),\n",
        "  MaxPool2D(pool_size=2),\n",
        "  Conv2D(filters=32, kernel_size=3, strides=1, padding=\"valid\", input_shape=(224, 224, 3), activation=\"relu\"),\n",
        "  MaxPool2D(pool_size=2),\n",
        "  Conv2D(filters=32, kernel_size=3, strides=1, padding=\"valid\", input_shape=(224, 224, 3), activation=\"relu\"),\n",
        "  MaxPool2D(pool_size=2),\n",
        "  Flatten(),\n",
        "  Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "id": "bherNl-9ek1y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xBjktctfTLy"
      },
      "source": [
        "model_6.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=Adam(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "id": "7xBjktctfTLy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDLb6mY3fhEh"
      },
      "source": [
        "history_6 = model_6.fit(\n",
        "    train_data_augmented,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_data_augmented),\n",
        "    validation_data=test_data,\n",
        "    validation_steps=len(test_data)\n",
        ")"
      ],
      "id": "WDLb6mY3fhEh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2hiDY7rgDGF"
      },
      "source": [
        "model_6.summary()"
      ],
      "id": "l2hiDY7rgDGF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7rGaUOhfzwi"
      },
      "source": [
        "plot_loss_curves(history_6)"
      ],
      "id": "a7rGaUOhfzwi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Of3eVgzgIox"
      },
      "source": [
        "plot_loss_curves(history_5)"
      ],
      "id": "0Of3eVgzgIox",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZzoqXplAZZ4"
      },
      "source": [
        "## Making a prediction with our trained model on our own custom data"
      ],
      "id": "fZzoqXplAZZ4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGsCqeC6hCla"
      },
      "source": [
        "print(class_names)"
      ],
      "id": "XGsCqeC6hCla",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydzThgTCAjpu"
      },
      "source": [
        "# View our example images\n",
        "\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg\n",
        "\n",
        "steak =  mpimg.imread(\"03-steak.jpeg\")\n",
        "plt.imshow(steak)\n",
        "plt.axis(False)"
      ],
      "id": "ydzThgTCAjpu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFx1ZlsaBEXW"
      },
      "source": [
        "steak.shape"
      ],
      "id": "TFx1ZlsaBEXW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A82yO7pOCaCG"
      },
      "source": [
        "# Create a function to import an image and resize it to be able to use with our image\n",
        "\n",
        "def load_and_prep_image(filename, img_shape=224):\n",
        "  # Read in the image\n",
        "  img = tf.io.read_file(filename)\n",
        "  # Decode the read file in to a tensor\n",
        "  img = tf.image.decode_image(img)\n",
        "  # Resize the image\n",
        "  img = tf.image.resize(img, size=[img_shape, img_shape])\n",
        "  # Rescale the image\n",
        "  img = img/255.\n",
        "  return img"
      ],
      "id": "A82yO7pOCaCG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc8baR6dBPu-"
      },
      "source": [
        "steak = load_and_prep_image(\"03-steak.jpeg\")"
      ],
      "id": "Rc8baR6dBPu-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH6EECibCGgW"
      },
      "source": [
        "steak"
      ],
      "id": "sH6EECibCGgW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sxkEP57CQdW"
      },
      "source": [
        "pred = model_6.predict(tf.expand_dims(steak, axis=0))\n",
        "pred"
      ],
      "id": "7sxkEP57CQdW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AKo71LSDysd"
      },
      "source": [
        "class_names"
      ],
      "id": "4AKo71LSDysd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LX5_N9HFGsO"
      },
      "source": [
        "pred_class = class_names[int(tf.round(pred))]\n",
        "pred_class"
      ],
      "id": "0LX5_N9HFGsO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9ZzMRxwFTnm"
      },
      "source": [
        "def pred_and_plot(model, filename, class_names=class_names):\n",
        "  # Import the traget image and preprocess it\n",
        "  img = load_and_prep_image(filename)\n",
        "\n",
        "  # make prediction\n",
        "  pred = model.predict(tf.expand_dims(img, axis=0))\n",
        "\n",
        "  # Get the prediction classs\n",
        "  pred_class = class_names[int(tf.round(pred))]\n",
        "  \n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"Prediction: {pred_class}\")\n",
        "  plt.axis(False)"
      ],
      "id": "h9ZzMRxwFTnm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isigp-CwFaNm"
      },
      "source": [
        "pred_and_plot(model_6, \"03-steak.jpeg\")"
      ],
      "id": "isigp-CwFaNm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1t1U5X3GjQY"
      },
      "source": [
        "!wget -O steak_new.jpg https://st.depositphotos.com/1000504/4511/i/950/depositphotos_45119209-stock-photo-grilled-beef-steak.jpg"
      ],
      "id": "c1t1U5X3GjQY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U36zE1IvGFlm"
      },
      "source": [
        "pred_and_plot(model_6, \"steak_new.jpg\")"
      ],
      "id": "U36zE1IvGFlm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuBPXD2HGaFm"
      },
      "source": [
        "!wget -O pizza_test.jpg https://www.oetker.in/Recipe/Recipes/oetker.in/in-en/pizza/image-thumb__52711__RecipeDetail/pizza-pollo-arrosto.jpg "
      ],
      "id": "NuBPXD2HGaFm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMR_EHlnHQVX"
      },
      "source": [
        "pred_and_plot(model_6, \"pizza_test.jpg\")"
      ],
      "id": "bMR_EHlnHQVX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyZqpGE3HTxe"
      },
      "source": [
        "## Multi-class Iamge Classification"
      ],
      "id": "tyZqpGE3HTxe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htm_vx90KM45"
      },
      "source": [
        "### 1. Explore data"
      ],
      "id": "htm_vx90KM45"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sJwvtUZKoqu"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip"
      ],
      "id": "2sJwvtUZKoqu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3DrZhTfK4E3"
      },
      "source": [
        "# Unzip the data\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"10_food_classes_all_data.zip\", \"r\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "id": "g3DrZhTfK4E3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSnfJQZ6LIzX"
      },
      "source": [
        "import os\n",
        "\n",
        "# Walk through data\n",
        "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_all_data\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in {dirpath}\")"
      ],
      "id": "tSnfJQZ6LIzX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl3qbCmOLkdI"
      },
      "source": [
        "!ls -la 10_food_classes_all_data/"
      ],
      "id": "Hl3qbCmOLkdI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8kTEBerLvgE"
      },
      "source": [
        "train_dir = \"10_food_classes_all_data/train/\"\n",
        "test_dir = \"10_food_classes_all_data/test/\""
      ],
      "id": "V8kTEBerLvgE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjdcvnO6MTw2"
      },
      "source": [
        "import pathlib\n",
        "import numpy as np\n",
        "\n",
        "data_dir = pathlib.Path(train_dir)\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\n",
        "print(class_names)"
      ],
      "id": "UjdcvnO6MTw2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1dYfGL5MIqv"
      },
      "source": [
        "# visualization\n",
        "import random\n",
        "img = view_random_image(target_dir=train_dir,\n",
        "                        target_class=random.choice(class_names))"
      ],
      "id": "q1dYfGL5MIqv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9STk6uAM2vB"
      },
      "source": [
        "### 2. Preprocess the data"
      ],
      "id": "v9STk6uAM2vB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2WqDVxxNXVg"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "# Load data from dirs and load it in batches\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\"\n",
        ")"
      ],
      "id": "_2WqDVxxNXVg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIRPRoDwO5ND"
      },
      "source": [
        "### 3. Create baseline model"
      ],
      "id": "jIRPRoDwO5ND"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTQo5DriPAlA"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Activation\n",
        "\n",
        "model_8 = Sequential([\n",
        "  Conv2D(32, 3, input_shape=(224, 224, 3)),\n",
        "  Activation(activation=\"relu\"),\n",
        "  Conv2D(32, 2, activation=\"relu\"),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(32, 3, activation=\"relu\"),\n",
        "  Conv2D(32, 3, activation=\"relu\"),\n",
        "  MaxPool2D(),\n",
        "  Flatten(),\n",
        "  Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model_8.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n"
      ],
      "id": "jTQo5DriPAlA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec-H12SnR7pR"
      },
      "source": [
        "\n",
        "history_8 = model_8.fit(\n",
        "    train_data,\n",
        "    steps_per_epoch=len(train_data),\n",
        "    epochs=10,\n",
        "    validation_data=test_data,\n",
        "    validation_steps=len(test_data)\n",
        ")"
      ],
      "id": "ec-H12SnR7pR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8Cs-hFMVyEC"
      },
      "source": [
        "### 5. Evaluate the model"
      ],
      "id": "K8Cs-hFMVyEC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmCa0iGKSN0R"
      },
      "source": [
        "plot_loss_curves(history_8)"
      ],
      "id": "xmCa0iGKSN0R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow1-apMCWFZJ"
      },
      "source": [
        "model_8.evaluate(test_data)"
      ],
      "id": "ow1-apMCWFZJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaz9ClW-WTXB"
      },
      "source": [
        "### 6. Adjust the model hyperparameters"
      ],
      "id": "uaz9ClW-WTXB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D21ES95KW-qJ"
      },
      "source": [
        "model_9  = Sequential([\n",
        "  Conv2D(32, 3, input_shape=(224, 224, 3)),\n",
        "  Activation(activation=\"relu\")  ,\n",
        "  MaxPool2D(),\n",
        "  Conv2D(32, 3, activation=\"relu\"),\n",
        "  MaxPool2D(),\n",
        "  Flatten(),\n",
        "  Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model_9.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "id": "D21ES95KW-qJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUmJMym7Y9ES"
      },
      "source": [
        "history_9 = model_9.fit(\n",
        "    train_data,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_data),\n",
        "    validation_data=test_data,\n",
        "    validation_steps=len(test_data)\n",
        ")"
      ],
      "id": "JUmJMym7Y9ES",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNlnPcJIZT7B"
      },
      "source": [
        "plot_loss_curves(history_9)"
      ],
      "id": "nNlnPcJIZT7B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3vUt_2nbvby"
      },
      "source": [
        "model_9.summary()"
      ],
      "id": "A3vUt_2nbvby",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is0rbRMlb47C"
      },
      "source": [
        "model_8.summary()"
      ],
      "id": "Is0rbRMlb47C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW7duy8ub77S"
      },
      "source": [
        "### Trying data augmentation"
      ],
      "id": "LW7duy8ub77S"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5g9vhwccTdq"
      },
      "source": [
        "# Create an augmented data generator instance\n",
        "train_datagen_aug = ImageDataGenerator(\n",
        "    rescale=1/255.,\n",
        "    rotation_range=0.2,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True     \n",
        ")\n",
        "\n",
        "train_data_aug = train_datagen_aug.flow_from_directory(train_dir,\n",
        "                                      target_size=(224, 224),\n",
        "                                      batch_size=32,\n",
        "                                      class_mode=\"categorical\")"
      ],
      "id": "F5g9vhwccTdq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcqdIKhpdOAr"
      },
      "source": [
        "# Create model with augmented data\n",
        "\n",
        "model_10 = tf.keras.models.clone_model(model_8)\n",
        "\n",
        "model_10.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n"
      ],
      "id": "UcqdIKhpdOAr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIxDRaO3eGwM"
      },
      "source": [
        "model_10.summary()"
      ],
      "id": "kIxDRaO3eGwM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyX-YSoheE-L"
      },
      "source": [
        "history_10 = model_10.fit(\n",
        "    train_data_aug,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_data_aug),\n",
        "    validation_data=test_data,\n",
        "    validation_steps=len(test_data)\n",
        ")"
      ],
      "id": "kyX-YSoheE-L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9f1bZYOeiTS"
      },
      "source": [
        "plot_loss_curves(history_10)"
      ],
      "id": "f9f1bZYOeiTS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmmQNPE9jk1t"
      },
      "source": [
        ""
      ],
      "id": "RmmQNPE9jk1t",
      "execution_count": null,
      "outputs": []
    }
  ]
}